{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse,os,wandb,tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import PIL.Image as Image\n",
    "from torchmetrics import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter Intilisation\n",
    "* Here we intialise the paramets such as epoch,batch size, train img size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train' #['test', 'train']\n",
    "checkpoint_folder ='checkpoint'\n",
    "model_type = 'PDCRN'\n",
    "train_path = '/media/hrishi/data/WORK/RESEARCH/2022/journal-2022/UDC/ds/Poled/train'\n",
    "test_path = '/media/hrishi/data/WORK/RESEARCH/2022/journal-2022/UDC/ds/Poled/val'\n",
    "batch_size = 1\n",
    "epochs = 1000\n",
    "LR = 1e-4\n",
    "num_filters  = 8\n",
    "dilation_rates = (3, 2, 1, 1, 1, 1)\n",
    "nPyramidFilters = 8\n",
    "log_name = 'logger'\n",
    "in_ch = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self,root_dir, is_train=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.is_train = is_train\n",
    "        self.hq_im_file_list = []\n",
    "        self.lq_im_file_list = []\n",
    "        for dir_path, _, file_names in os.walk(root_dir):\n",
    "            for f_paths in sorted(file_names):\n",
    "                if dir_path.endswith('HQ'):\n",
    "                    self.hq_im_file_list.append(os.path.join(dir_path,f_paths))\n",
    "                elif dir_path.endswith('LQ'):\n",
    "                    self.lq_im_file_list.append(os.path.join(dir_path,f_paths))\n",
    "        self.tensor_convert = T.ToTensor()\n",
    "        self.train_transform = T.Compose(\n",
    "    [T.RandomVerticalFlip(p=0.5),T.RandomHorizontalFlip(p=0.5)\\\n",
    "       ,T.RandomAffine((0,360)),T.Resize((256,256)),T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "        self.val_transform = T.Compose([T.Resize((256,256)),T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    def __len__(self):\n",
    "        return len(self.hq_im_file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_hq_fname = self.hq_im_file_list[idx]\n",
    "        image_lq_fname = self.lq_im_file_list[idx]\n",
    "        hq_image = self.tensor_convert(Image.open(image_hq_fname).convert('RGB')).unsqueeze(dim=0)\n",
    "        lq_image = self.tensor_convert(Image.open(image_lq_fname).convert('RGB')).unsqueeze(dim=0)\n",
    "        concat_img = torch.cat([hq_image,lq_image],dim=0)\n",
    "        if self.is_train:\n",
    "            image = self.train_transform(concat_img)\n",
    "        else:\n",
    "            image = self.val_transform(concat_img)\n",
    "        hq_img,lq_img = image.tensor_split(2)\n",
    "        return lq_img.squeeze(0),hq_img.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Custom_Dataset(train_path,is_train=True)\n",
    "train_dataloader = DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=os.cpu_count())\n",
    "val_ds = Custom_Dataset(test_path,is_train=False)\n",
    "val_dataloader = DataLoader(val_ds,batch_size=batch_size,shuffle=False,num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DWT and IWT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DWT,self).__init__()\n",
    "    def forward(self,input):\n",
    "        x01 = input[:,:,0::2,:] / 4.0\n",
    "        x02 = input[:,:,1::2,:] / 4.0\n",
    "        x1 = x01[:, :,:, 0::2]\n",
    "        x2 = x01[:, :,:, 1::2]\n",
    "        x3 = x02[:, :,:, 0::2]\n",
    "        x4 = x02[:, :,:, 1::2]\n",
    "        y1 = x1+x2+x3+x4\n",
    "        y2 = x1-x2+x3-x4\n",
    "        y3 = x1+x2-x3-x4\n",
    "        y4 = x1-x2-x3+x4\n",
    "        return torch.cat([y1, y2, y3, y4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IWT(nn.Module):\n",
    "    def __init__(self,scale=2):\n",
    "        super(IWT,self).__init__()\n",
    "        self.upsampler = nn.PixelShuffle(scale)\n",
    "\n",
    "    def kernel_build(self,input_shape):\n",
    "        c = input_shape[1]\n",
    "        out_c = c >> 2\n",
    "        kernel = np.zeros((c, c,1, 1), dtype=np.float32)\n",
    "        for i in range(0, c, 4):\n",
    "            idx = i >> 2\n",
    "            kernel[idx,idx::out_c,0,0]          = [1, 1, 1, 1]\n",
    "            kernel[idx+out_c,idx::out_c,0,0]    = [1,-1, 1,-1]\n",
    "            kernel[idx+out_c*2,idx::out_c,0,0]  = [1, 1,-1,-1]\n",
    "            kernel[idx+out_c*3,idx::out_c,0,0]  = [1,-1,-1, 1]\n",
    "        self.kernel = torch.tensor(data=kernel,dtype=torch.float32,requires_grad=True).to(device)\n",
    "        return None\n",
    "\n",
    "    def forward(self,input):\n",
    "        self.kernel_build(input.shape)\n",
    "        y = nn.functional.conv2d(input,weight = self.kernel, padding='same')\n",
    "        y = self.upsampler(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Components and Model Intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilationPyramid(nn.Module):\n",
    "    def __init__(self,num_filters,dilation_rates):\n",
    "        super(DilationPyramid,self).__init__()\n",
    "        self.layer_1 = nn.Conv2d(num_filters,num_filters*2,3,padding='same')\n",
    "        self.layer_2 = nn.Conv2d(num_filters*2,num_filters,3,padding='same',dilation=dilation_rates[0])\n",
    "        self.layer_3 = nn.Sequential(*[nn.Conv2d(num_filters,num_filters,3,padding='same',dilation=dil_rate) \\\n",
    "            for dil_rate in dilation_rates[1:]])\n",
    "        self.layer_4 = nn.Conv2d(num_filters*2,num_filters,1,padding='same')\n",
    "    def forward(self,input):\n",
    "        x = self.layer_1(input)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.cat([input,x],dim=1)\n",
    "        out = self.layer_4(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidBlock(nn.Module):\n",
    "    def __init__(self,num_filters,dilation_rates,nPyramidFilters):\n",
    "        super(PyramidBlock,self).__init__()\n",
    "        self.feat_extract = nn.Sequential(*[DilationPyramid(nPyramidFilters,dilation_rates),\\\n",
    "        nn.Conv2d(num_filters,num_filters,3,padding='same')])\n",
    "    def forward(self,input):\n",
    "        x = self.feat_extract(input)*0.1\n",
    "        return input+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDC_Arc(nn.Module):\n",
    "    def __init__(self,in_ch,num_filters,dilation_rates,nPyramidFilters):\n",
    "        super(UDC_Arc,self).__init__()\n",
    "        self.encoder = nn.Sequential(*nn.ModuleList([DWT(),nn.PixelUnshuffle(downscale_factor=2),\\\n",
    "        nn.Conv2d(in_channels=in_ch*4*4,out_channels=num_filters,kernel_size=5,padding='same'),\\\n",
    "        nn.Conv2d(in_channels=num_filters,out_channels=num_filters,kernel_size=3,padding='same'),\\\n",
    "        PyramidBlock(num_filters,dilation_rates,nPyramidFilters),\\\n",
    "        nn.Conv2d(num_filters,num_filters*2,kernel_size=5,stride=2,padding=2),\\\n",
    "        PyramidBlock(num_filters*2,dilation_rates,nPyramidFilters*2),\\\n",
    "        nn.Conv2d(num_filters*2,num_filters*4,kernel_size=5,stride=2,padding=2),\\\n",
    "        PyramidBlock(num_filters*4,dilation_rates,nPyramidFilters*4)\n",
    "        ]))\n",
    "        self.decoder = nn.Sequential(*nn.ModuleList([PyramidBlock(num_filters*4,dilation_rates,nPyramidFilters*4),\\\n",
    "        nn.ConvTranspose2d(num_filters*4,num_filters*2,kernel_size = 4,stride=2,padding=1),\\\n",
    "        PyramidBlock(num_filters*2,dilation_rates,nPyramidFilters*2),\\\n",
    "        nn.ConvTranspose2d(num_filters*2,num_filters,kernel_size = 4,stride=2,padding=1),\\\n",
    "        PyramidBlock(num_filters,dilation_rates,nPyramidFilters),\\\n",
    "        nn.PixelShuffle(upscale_factor=2),nn.Conv2d(num_filters//4,in_ch*4,3,padding='same'),IWT(),\\\n",
    "        nn.Tanh()\n",
    "        ]))\n",
    "    def forward(self,input):\n",
    "        x_enc = self.encoder(input)\n",
    "        x_dec = self.decoder(x_enc)\n",
    "        return x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UDC_Arc(in_ch,num_filters,dilation_rates,nPyramidFilters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(type='last'):\n",
    "    checkpoint_folder = os.path.join(checkpoint_folder,model_type)\n",
    "    if not os.path.exists(checkpoint_folder):\n",
    "        os.makedirs(checkpoint_folder)\n",
    "    checkpoint_filename = os.path.join(checkpoint_folder,f'{type}.pth')\n",
    "    save_data = {\n",
    "        'step': current_epoch,\n",
    "        f'best_psnr':best_psnr,\n",
    "        f'best_ssim':best_ssim,\n",
    "        'generator_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(save_data, checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint_for_training(checkpoint_folder,model_type,type ='last'):\n",
    "    checkpoint_folder = os.path.join(checkpoint_folder,model_type)\n",
    "    checkpoint_filename = os.path.join(checkpoint_folder, f'{type}.pth')\n",
    "    if not os.path.exists(checkpoint_filename):\n",
    "        print(\"Couldn't find checkpoint file. Starting training from the beginning.\")\n",
    "        return\n",
    "    data = torch.load(checkpoint_filename)\n",
    "    current_epoch = data['step']\n",
    "    best_psnr = data['best_psnr']\n",
    "    best_ssim = data['best_ssim']\n",
    "    model.load_state_dict(data['generator_state_dict'])\n",
    "    optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "    print(f\"Restored model at epoch {current_epoch}.\")\n",
    "    return best_psnr,best_ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hrishi/data/envs/miniconda3/envs/torchenv/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SSIM` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "current_epoch = 0\n",
    "best_psnr = 0\n",
    "best_ssim = 0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=(batch_size*0.3)/256, momentum=0.9)\n",
    "criterion = torch.nn.L1Loss().to(device)\n",
    "psnr  = PeakSignalNoiseRatio().to(device)\n",
    "ssim = StructuralSimilarityIndexMeasure().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    for count,(inputs, gt) in enumerate(tqdm.tqdm(train_dataloader)):\n",
    "        inputs = inputs.to(device)\n",
    "        gt = gt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,gt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    wandb.log({'train_l1_loss':loss.item()})\n",
    "    wandb.log({'Learning rate':optimizer.param_groups[0]['lr']})\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch():\n",
    "    model.eval()\n",
    "    for inputs, gt in tqdm.tqdm(val_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        gt = gt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _ = criterion(outputs,gt)\n",
    "        psnr.update(outputs,gt)\n",
    "        ssim.update(outputs,gt)\n",
    "    wandb.log({'val_psnr':psnr.compute().item()})\n",
    "    wandb.log({'val_ssim':ssim.compute().item()})\n",
    "    val_psnr,val_ssim = psnr.compute().item(),ssim.compute().item()\n",
    "    psnr.reset()\n",
    "    ssim.reset()\n",
    "    if val_psnr>best_psnr:\n",
    "        best_psnr = val_psnr\n",
    "        save_checkpoint('best')\n",
    "    else:\n",
    "        save_checkpoint('last')\n",
    "    if val_ssim>best_ssim:\n",
    "        best_ssim = val_ssim\n",
    "    print(f'Epoch = {current_epoch} Val best PSNR = {best_psnr},Val best SSIM= {best_ssim},\\\n",
    "        Val current PSNR = {val_psnr},Val currentSSIM= {val_ssim}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'checkpoint_folder' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6155/4119280018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_model_checkpoint_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6155/2507676296.py\u001b[0m in \u001b[0;36mload_model_checkpoint_for_training\u001b[0;34m(type)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_checkpoint_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcheckpoint_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcheckpoint_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{type}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Couldn't find checkpoint file. Starting training from the beginning.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'checkpoint_folder' referenced before assignment"
     ]
    }
   ],
   "source": [
    "load_model_checkpoint_for_training()\n",
    "for epoch in range(current_epoch,epochs):\n",
    "    current_epoch = epoch\n",
    "    train_epoch()\n",
    "    val_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9224ef34cb0a5a9f529319a8f3c472af375e732c0f5a2408c12cad5c4f84ce0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

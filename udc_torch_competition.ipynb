{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjnNY4UvZ9a9"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install wandb\n",
        "!pip install torchsummary\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49jiz431yh6l"
      },
      "source": [
        "# Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f-5nQCyhbIj"
      },
      "outputs": [],
      "source": [
        "!gdown 1bbIy5P_aP_xarxUw4yPuPk4CglHPGLco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ljt17_ihehJ"
      },
      "outputs": [],
      "source": [
        "!gdown 1JH7CQ77wjkV0Xnp14l8u4bqJF0ENGMqp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3iTo-cEj7HK"
      },
      "outputs": [],
      "source": [
        "!unzip UDC_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSLshZpbCkQb"
      },
      "outputs": [],
      "source": [
        "!unzip UDC_validation_input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPdCevJwmbJJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('ds/val/GT',exist_ok = True)\n",
        "os.makedirs('ds/val/input',exist_ok = True)\n",
        "os.makedirs('ds/train/GT',exist_ok = True)\n",
        "os.makedirs('ds/train/input',exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT_sAC8UoxHn"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "val_list = sorted(os.listdir('training/GT/'))[:26]\n",
        "for im_name in val_list:\n",
        "  shutil.move(f'training/GT/{im_name}',f'ds/val/GT/{im_name}')\n",
        "for im_name in val_list:\n",
        "  shutil.move(f'training/input/{im_name}',f'ds/val/input/{im_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwexp_whsoZC"
      },
      "outputs": [],
      "source": [
        "train_list = sorted(os.listdir('training/GT/'))\n",
        "for im_name in train_list:\n",
        "  shutil.move(f'training/GT/{im_name}',f'ds/train/GT/{im_name}')\n",
        "for im_name in train_list:\n",
        "  shutil.move(f'training/input/{im_name}',f'ds/train/input/{im_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIx9No4jaOrK"
      },
      "source": [
        "##### Library import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TJCaH25saGGA"
      },
      "outputs": [],
      "source": [
        "import argparse,os,wandb,tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import PIL.Image as Image\n",
        "from torchmetrics import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOsPxcNcaT1X"
      },
      "source": [
        "##### Parameter Intilisation\n",
        "* Here we intialise the paramets such as epoch,batch size, train img size etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n2gTfJ7PaRyL"
      },
      "outputs": [],
      "source": [
        "mode = 'train' #['test', 'train']\n",
        "checkpoint_folder ='PDCRN_SYNTH'\n",
        "model_type = 'PDCRN'\n",
        "train_path = 'ds/train'\n",
        "test_path = 'ds/val'\n",
        "batch_size = 48\n",
        "epochs = 1000\n",
        "LR = 1e-4\n",
        "num_filters  = 64\n",
        "dilation_rates = (3, 2, 1, 1, 1, 1)\n",
        "nPyramidFilters = 64\n",
        "log_name = 'logger'\n",
        "in_ch = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xyS7DhhaXE-"
      },
      "source": [
        "##### Dataset Intialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucDaB-ChaWY6"
      },
      "outputs": [],
      "source": [
        "class Custom_Dataset(Dataset):\n",
        "    def __init__(self,root_dir, is_train=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.is_train = is_train\n",
        "        self.hq_im_file_list = []\n",
        "        self.lq_im_file_list = []\n",
        "        self.hq_train_files = {}\n",
        "        self.lq_train_files = {}\n",
        "        for dir_path, _, file_names in os.walk(root_dir):\n",
        "            for f_paths in sorted(file_names):\n",
        "                if dir_path.endswith('GT'):\n",
        "                    self.hq_im_file_list.append(os.path.join(dir_path,f_paths))\n",
        "                elif dir_path.endswith('input'):\n",
        "                    self.lq_im_file_list.append(os.path.join(dir_path,f_paths))\n",
        "        for im_names in self.hq_im_file_list:\n",
        "            self.hq_train_files[im_names] = np.load(im_names)\n",
        "        for im_names in self.lq_im_file_list:\n",
        "            self.lq_train_files[im_names] = np.load(im_names)\n",
        "        # self.tensor_convert = T.ToTensor()\n",
        "        self.train_transform = T.Compose(\n",
        "    [T.RandomCrop((512,512)),T.RandomVerticalFlip(p=0.5),T.RandomHorizontalFlip(p=0.5)\\\n",
        "       ,T.RandomAffine((0,360))])\n",
        "\n",
        "        # self.val_transform = T.Compose([T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    def __len__(self):\n",
        "        return len(self.hq_im_file_list)\n",
        "    def tone_transform(self,im,c=0.25):\n",
        "      mapped_x = im / (im + c)\n",
        "      return mapped_x\n",
        "    def __getitem__(self, idx):\n",
        "        image_hq_fname = self.hq_train_files[self.hq_im_file_list[idx]]\n",
        "        image_lq_fname = self.lq_train_files[self.lq_im_file_list[idx]]\n",
        "        hq_image = torch.from_numpy(self.tone_transform(image_hq_fname)).unsqueeze(dim=0).permute(0,3,1,2)\n",
        "        lq_image = torch.from_numpy(self.tone_transform(image_lq_fname)).unsqueeze(dim=0).permute(0,3,1,2)\n",
        "        concat_img = torch.cat([hq_image,lq_image],dim=0)\n",
        "        if self.is_train:\n",
        "            image = self.train_transform(concat_img)\n",
        "        else:\n",
        "            # image = self.val_transform(concat_img)\n",
        "            image = concat_img\n",
        "        hq_img,lq_img = image.tensor_split(2)\n",
        "        return lq_img.squeeze(0),hq_img.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtroVaMValU5"
      },
      "outputs": [],
      "source": [
        "train_ds = Custom_Dataset(train_path,is_train=True)\n",
        "train_dataloader = DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=8)\n",
        "val_ds = Custom_Dataset(test_path,is_train=False)\n",
        "val_dataloader = DataLoader(val_ds,batch_size=1,shuffle=False,num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRMOPpw4arIq"
      },
      "source": [
        "##### DWT and IWT layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "14TCHFuzaoHM"
      },
      "outputs": [],
      "source": [
        "class DWT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DWT,self).__init__()\n",
        "    def forward(self,input):\n",
        "        x01 = input[:,:,0::2,:] / 4.0\n",
        "        x02 = input[:,:,1::2,:] / 4.0\n",
        "        x1 = x01[:, :,:, 0::2]\n",
        "        x2 = x01[:, :,:, 1::2]\n",
        "        x3 = x02[:, :,:, 0::2]\n",
        "        x4 = x02[:, :,:, 1::2]\n",
        "        y1 = x1+x2+x3+x4\n",
        "        y2 = x1-x2+x3-x4\n",
        "        y3 = x1+x2-x3-x4\n",
        "        y4 = x1-x2-x3+x4\n",
        "        return torch.cat([y1, y2, y3, y4], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_iB1SQcoatEm"
      },
      "outputs": [],
      "source": [
        "class IWT(nn.Module):\n",
        "    def __init__(self,scale=2):\n",
        "        super(IWT,self).__init__()\n",
        "        self.upsampler = nn.PixelShuffle(scale)\n",
        "\n",
        "    def kernel_build(self,input_shape):\n",
        "        c = input_shape[1]\n",
        "        out_c = c >> 2\n",
        "        kernel = np.zeros((c, c,1, 1), dtype=np.float32)\n",
        "        for i in range(0, c, 4):\n",
        "            idx = i >> 2\n",
        "            kernel[idx,idx::out_c,0,0]          = [1, 1, 1, 1]\n",
        "            kernel[idx+out_c,idx::out_c,0,0]    = [1,-1, 1,-1]\n",
        "            kernel[idx+out_c*2,idx::out_c,0,0]  = [1, 1,-1,-1]\n",
        "            kernel[idx+out_c*3,idx::out_c,0,0]  = [1,-1,-1, 1]\n",
        "        self.kernel = torch.tensor(data=kernel,dtype=torch.float32,requires_grad=True).to(device)\n",
        "        return None\n",
        "\n",
        "    def forward(self,input):\n",
        "        self.kernel_build(input.shape)\n",
        "        y = nn.functional.conv2d(input,weight = self.kernel, padding='same')\n",
        "        y = self.upsampler(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoMepuWLaxXD"
      },
      "source": [
        "#### Model Components and Model Intialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_Cf0_OM3avTW"
      },
      "outputs": [],
      "source": [
        "class DilationPyramid(nn.Module):\n",
        "    def __init__(self,num_filters,dilation_rates):\n",
        "        super(DilationPyramid,self).__init__()\n",
        "        self.layer_1 = nn.Conv2d(num_filters,num_filters*2,3,padding='same')\n",
        "        self.layer_2 = nn.Conv2d(num_filters*2,num_filters,3,padding='same',dilation=dilation_rates[0])\n",
        "        self.layer_3 = []\n",
        "        for dil_rate in dilation_rates[1:]:\n",
        "          self.layer_3.append(nn.Conv2d(num_filters,num_filters,3,padding='same',dilation=dil_rate))\n",
        "          self.layer_3.append(nn.ReLU())\n",
        "        self.layer_3 = nn.Sequential(*self.layer_3)\n",
        "        # self.layer_3 = nn.Sequential(*[[nn.Conv2d(num_filters,num_filters,3,padding='same',dilation=dil_rate),nn.ReLU()] for dil_rate in dilation_rates[1:]])\n",
        "        self.layer_4 = nn.Conv2d(num_filters*2,num_filters,1,padding='same')\n",
        "    def forward(self,input):\n",
        "        x = self.layer_1(input)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = torch.cat([input,x],dim=1)\n",
        "        x = self.layer_4(x)\n",
        "        out = nn.functional.relu(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "icnF5kADazPv"
      },
      "outputs": [],
      "source": [
        "class PyramidBlock(nn.Module):\n",
        "    def __init__(self,num_filters,dilation_rates,nPyramidFilters):\n",
        "        super(PyramidBlock,self).__init__()\n",
        "        self.feat_extract = nn.Sequential(*[DilationPyramid(nPyramidFilters,dilation_rates),\\\n",
        "        nn.Conv2d(num_filters,num_filters,3,padding='same')])\n",
        "    def forward(self,input):\n",
        "        x = self.feat_extract(input)*0.1\n",
        "        return input+x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ai8B5lXIa04v"
      },
      "outputs": [],
      "source": [
        "class UDC_Arc(nn.Module):\n",
        "    def __init__(self,in_ch,num_filters,dilation_rates,nPyramidFilters):\n",
        "        super(UDC_Arc,self).__init__()\n",
        "        self.encoder = nn.Sequential(*nn.ModuleList([DWT(),nn.PixelUnshuffle(downscale_factor=2),\\\n",
        "        nn.Conv2d(in_channels=in_ch*4*4,out_channels=num_filters,kernel_size=5,padding='same'),\\\n",
        "        nn.Conv2d(in_channels=num_filters,out_channels=num_filters,kernel_size=3,padding='same'),\\\n",
        "        PyramidBlock(num_filters,dilation_rates,nPyramidFilters),\\\n",
        "        nn.Conv2d(num_filters,num_filters*2,kernel_size=5,stride=2,padding=2),\\\n",
        "        PyramidBlock(num_filters*2,dilation_rates,nPyramidFilters*2),\\\n",
        "        nn.Conv2d(num_filters*2,num_filters*4,kernel_size=5,stride=2,padding=2),\\\n",
        "        PyramidBlock(num_filters*4,dilation_rates,nPyramidFilters*4)\n",
        "        ]))\n",
        "        self.decoder = nn.Sequential(*nn.ModuleList([PyramidBlock(num_filters*4,dilation_rates,nPyramidFilters*4),\\\n",
        "        nn.ConvTranspose2d(num_filters*4,num_filters*2,kernel_size = 4,stride=2,padding=1),\\\n",
        "        PyramidBlock(num_filters*2,dilation_rates,nPyramidFilters*2),\\\n",
        "        nn.ConvTranspose2d(num_filters*2,num_filters,kernel_size = 4,stride=2,padding=1),\\\n",
        "        PyramidBlock(num_filters,dilation_rates,nPyramidFilters),\\\n",
        "        nn.PixelShuffle(upscale_factor=2),nn.Conv2d(num_filters//4,in_ch*4,3,padding='same'),IWT(),\\\n",
        "        # nn.Tanh()\n",
        "        ]))\n",
        "    def forward(self,input):\n",
        "        x_enc = self.encoder(input)\n",
        "        x_dec = self.decoder(x_enc)\n",
        "        x_dec = torch.minimum(torch.maximum(torch.zeros_like(x_dec),x_dec),torch.ones_like(x_dec)*500)\n",
        "        # x_dec = torch.nn.functional.relu(x_dec)\n",
        "        return x_dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WE1f7XkDa3SJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "               DWT-1         [-1, 12, 128, 128]               0\n",
            "    PixelUnshuffle-2           [-1, 48, 64, 64]               0\n",
            "            Conv2d-3           [-1, 64, 64, 64]          76,864\n",
            "            Conv2d-4           [-1, 64, 64, 64]          36,928\n",
            "            Conv2d-5          [-1, 128, 64, 64]          73,856\n",
            "            Conv2d-6           [-1, 64, 64, 64]          73,792\n",
            "            Conv2d-7           [-1, 64, 64, 64]          36,928\n",
            "              ReLU-8           [-1, 64, 64, 64]               0\n",
            "            Conv2d-9           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-10           [-1, 64, 64, 64]               0\n",
            "           Conv2d-11           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-12           [-1, 64, 64, 64]               0\n",
            "           Conv2d-13           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-14           [-1, 64, 64, 64]               0\n",
            "           Conv2d-15           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-16           [-1, 64, 64, 64]               0\n",
            "           Conv2d-17           [-1, 64, 64, 64]           8,256\n",
            "  DilationPyramid-18           [-1, 64, 64, 64]               0\n",
            "           Conv2d-19           [-1, 64, 64, 64]          36,928\n",
            "     PyramidBlock-20           [-1, 64, 64, 64]               0\n",
            "           Conv2d-21          [-1, 128, 32, 32]         204,928\n",
            "           Conv2d-22          [-1, 256, 32, 32]         295,168\n",
            "           Conv2d-23          [-1, 128, 32, 32]         295,040\n",
            "           Conv2d-24          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-25          [-1, 128, 32, 32]               0\n",
            "           Conv2d-26          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-27          [-1, 128, 32, 32]               0\n",
            "           Conv2d-28          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-29          [-1, 128, 32, 32]               0\n",
            "           Conv2d-30          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-31          [-1, 128, 32, 32]               0\n",
            "           Conv2d-32          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-33          [-1, 128, 32, 32]               0\n",
            "           Conv2d-34          [-1, 128, 32, 32]          32,896\n",
            "  DilationPyramid-35          [-1, 128, 32, 32]               0\n",
            "           Conv2d-36          [-1, 128, 32, 32]         147,584\n",
            "     PyramidBlock-37          [-1, 128, 32, 32]               0\n",
            "           Conv2d-38          [-1, 256, 16, 16]         819,456\n",
            "           Conv2d-39          [-1, 512, 16, 16]       1,180,160\n",
            "           Conv2d-40          [-1, 256, 16, 16]       1,179,904\n",
            "           Conv2d-41          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-42          [-1, 256, 16, 16]               0\n",
            "           Conv2d-43          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-44          [-1, 256, 16, 16]               0\n",
            "           Conv2d-45          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-46          [-1, 256, 16, 16]               0\n",
            "           Conv2d-47          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-48          [-1, 256, 16, 16]               0\n",
            "           Conv2d-49          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-50          [-1, 256, 16, 16]               0\n",
            "           Conv2d-51          [-1, 256, 16, 16]         131,328\n",
            "  DilationPyramid-52          [-1, 256, 16, 16]               0\n",
            "           Conv2d-53          [-1, 256, 16, 16]         590,080\n",
            "     PyramidBlock-54          [-1, 256, 16, 16]               0\n",
            "           Conv2d-55          [-1, 512, 16, 16]       1,180,160\n",
            "           Conv2d-56          [-1, 256, 16, 16]       1,179,904\n",
            "           Conv2d-57          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-58          [-1, 256, 16, 16]               0\n",
            "           Conv2d-59          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-60          [-1, 256, 16, 16]               0\n",
            "           Conv2d-61          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-62          [-1, 256, 16, 16]               0\n",
            "           Conv2d-63          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-64          [-1, 256, 16, 16]               0\n",
            "           Conv2d-65          [-1, 256, 16, 16]         590,080\n",
            "             ReLU-66          [-1, 256, 16, 16]               0\n",
            "           Conv2d-67          [-1, 256, 16, 16]         131,328\n",
            "  DilationPyramid-68          [-1, 256, 16, 16]               0\n",
            "           Conv2d-69          [-1, 256, 16, 16]         590,080\n",
            "     PyramidBlock-70          [-1, 256, 16, 16]               0\n",
            "  ConvTranspose2d-71          [-1, 128, 32, 32]         524,416\n",
            "           Conv2d-72          [-1, 256, 32, 32]         295,168\n",
            "           Conv2d-73          [-1, 128, 32, 32]         295,040\n",
            "           Conv2d-74          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-75          [-1, 128, 32, 32]               0\n",
            "           Conv2d-76          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-77          [-1, 128, 32, 32]               0\n",
            "           Conv2d-78          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-79          [-1, 128, 32, 32]               0\n",
            "           Conv2d-80          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-81          [-1, 128, 32, 32]               0\n",
            "           Conv2d-82          [-1, 128, 32, 32]         147,584\n",
            "             ReLU-83          [-1, 128, 32, 32]               0\n",
            "           Conv2d-84          [-1, 128, 32, 32]          32,896\n",
            "  DilationPyramid-85          [-1, 128, 32, 32]               0\n",
            "           Conv2d-86          [-1, 128, 32, 32]         147,584\n",
            "     PyramidBlock-87          [-1, 128, 32, 32]               0\n",
            "  ConvTranspose2d-88           [-1, 64, 64, 64]         131,136\n",
            "           Conv2d-89          [-1, 128, 64, 64]          73,856\n",
            "           Conv2d-90           [-1, 64, 64, 64]          73,792\n",
            "           Conv2d-91           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-92           [-1, 64, 64, 64]               0\n",
            "           Conv2d-93           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-94           [-1, 64, 64, 64]               0\n",
            "           Conv2d-95           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-96           [-1, 64, 64, 64]               0\n",
            "           Conv2d-97           [-1, 64, 64, 64]          36,928\n",
            "             ReLU-98           [-1, 64, 64, 64]               0\n",
            "           Conv2d-99           [-1, 64, 64, 64]          36,928\n",
            "            ReLU-100           [-1, 64, 64, 64]               0\n",
            "          Conv2d-101           [-1, 64, 64, 64]           8,256\n",
            " DilationPyramid-102           [-1, 64, 64, 64]               0\n",
            "          Conv2d-103           [-1, 64, 64, 64]          36,928\n",
            "    PyramidBlock-104           [-1, 64, 64, 64]               0\n",
            "    PixelShuffle-105         [-1, 16, 128, 128]               0\n",
            "          Conv2d-106         [-1, 12, 128, 128]           1,740\n",
            "    PixelShuffle-107          [-1, 3, 256, 256]               0\n",
            "             IWT-108          [-1, 3, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 17,631,372\n",
            "Trainable params: 17,631,372\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 137.00\n",
            "Params size (MB): 67.26\n",
            "Estimated Total Size (MB): 205.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = UDC_Arc(in_ch,num_filters,dilation_rates,nPyramidFilters)\n",
        "model = model.to(device)\n",
        "summary(model,input_size=(3,256,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtiEg3URbP9t"
      },
      "source": [
        "#### Save and Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH38Eqyia404"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(checkpoint_folder,model_type,type='last'):\n",
        "    checkpoint_folder = os.path.join(checkpoint_folder,model_type)\n",
        "    if not os.path.exists(checkpoint_folder):\n",
        "        os.makedirs(checkpoint_folder)\n",
        "    checkpoint_filename = os.path.join(checkpoint_folder,f'{type}.pth')\n",
        "    save_data = {\n",
        "        'step': current_epoch,\n",
        "        f'best_psnr':best_psnr,\n",
        "        f'best_ssim':best_ssim,\n",
        "        'generator_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(save_data, checkpoint_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9Om5pdbXnH"
      },
      "outputs": [],
      "source": [
        "def load_model_checkpoint_for_training(checkpoint_folder,model_type,type ='last'):\n",
        "    checkpoint_folder = os.path.join(checkpoint_folder,model_type)\n",
        "    checkpoint_filename = os.path.join(checkpoint_folder, f'{type}.pth')\n",
        "    if not os.path.exists(checkpoint_filename):\n",
        "        print(\"Couldn't find checkpoint file. Starting training from the beginning.\")\n",
        "        return 0,0,0\n",
        "    data = torch.load(checkpoint_filename)\n",
        "    current_epoch = data['step']\n",
        "    best_psnr = data['best_psnr']\n",
        "    best_ssim = data['best_ssim']\n",
        "    model.load_state_dict(data['generator_state_dict'])\n",
        "    optimizer.load_state_dict(data['optimizer_state_dict'])\n",
        "    print(f\"Restored model at epoch {current_epoch}.\")\n",
        "    return best_psnr,best_ssim,current_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLZyq9uGbbYF"
      },
      "source": [
        "#### Train and validation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZtb872cbZWO"
      },
      "outputs": [],
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    for count,(inputs, gt) in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "        inputs = inputs.to(device)\n",
        "        gt = gt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs,gt)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    wandb.log({'train_l1_loss':loss.item()})\n",
        "    wandb.log({'Learning rate':optimizer.param_groups[0]['lr']})\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAMMNDR1bdvA"
      },
      "outputs": [],
      "source": [
        "def val_epoch(checkpoint_folder,model_type,best_psnr,best_ssim):\n",
        "    model.eval()\n",
        "    for inputs, gt in tqdm.tqdm(val_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        gt = gt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            _ = criterion(outputs,gt)\n",
        "        psnr.update(outputs,gt)\n",
        "        # ssim.update(outputs,gt)\n",
        "    wandb.log({'val_psnr':psnr.compute().item()})\n",
        "    wandb.log({'val_ssim':ssim.compute().item()})\n",
        "    val_psnr,val_ssim = psnr.compute().item(),ssim.compute().item()\n",
        "    psnr.reset()\n",
        "    ssim.reset()\n",
        "    if val_psnr>best_psnr:\n",
        "        best_psnr = val_psnr\n",
        "        save_checkpoint(checkpoint_folder,model_type,'best')\n",
        "    else:\n",
        "        save_checkpoint(checkpoint_folder,model_type,'last')\n",
        "    if val_ssim>best_ssim:\n",
        "        best_ssim = val_ssim\n",
        "    print(f'Epoch = {current_epoch} Val best PSNR = {best_psnr},Val best SSIM= {best_ssim},Val current PSNR = {val_psnr},Val currentSSIM= {val_ssim}')\n",
        "    return best_psnr,best_ssim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-In5_u2bhSE"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPbRXr3vbfe4"
      },
      "outputs": [],
      "source": [
        "current_epoch = 0\n",
        "best_psnr = 0\n",
        "best_ssim = 0\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=(batch_size*0.3)/256, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\n",
        "criterion = torch.nn.L1Loss().to(device)\n",
        "psnr  = PeakSignalNoiseRatio().to(device)\n",
        "ssim = StructuralSimilarityIndexMeasure().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow3czNE8bjDg"
      },
      "outputs": [],
      "source": [
        "best_psnr,best_ssim,current_epoch = load_model_checkpoint_for_training(checkpoint_folder,model_type)\n",
        "wandb.init(project=f\"UDC\",name=log_name)\n",
        "for epoch in range(current_epoch,epochs):\n",
        "    current_epoch = epoch\n",
        "    train_epoch()\n",
        "    best_psnr,best_ssim = val_epoch(checkpoint_folder,model_type,best_psnr,best_ssim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcV0NrqbO4b5"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6jEqQV5HxKGo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Gukz3B0mO4b5"
      },
      "outputs": [],
      "source": [
        "def test(inp):\n",
        "    output = model(inp)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inv_tone_transform(mapped_x,c=0.25):\n",
        "    # im = (mapped_x*c)/(1-mapped_x)\n",
        "    im = np.divide((mapped_x*c),(1-mapped_x), out=mapped_x, where=mapped_x!=1)\n",
        "    # im = np.where(im== float('inf'),500.0,im)\n",
        "    return im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tone_transform(im,c=0.25):\n",
        "    mapped_x = im / (im + c)\n",
        "    return mapped_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "piSNkk06O4b6"
      },
      "outputs": [],
      "source": [
        "test_dir = 'validation/input'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from model.DBWN import DBWN\n",
        "# test_model = DBWN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tlCzhi9BO4b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_model_folder = os.path.join(checkpoint_folder,model_type)\n",
        "checkpoint_filename = os.path.join(checkpoint_model_folder, 'best.pth')\n",
        "# checkpoint_filename = '/mnt/c/Users/Hrishikesh/Desktop/hrishi/WORK/RESEARCH/2022/journal-2022/UDC/PDCRN_UDC/checkpoint_densen/best.pth'\n",
        "data = torch.load(checkpoint_filename)\n",
        "model.load_state_dict(data['generator_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b9ee6-lgO4b7"
      },
      "outputs": [],
      "source": [
        "image_list = []\n",
        "for dir_path, _, file_names in os.walk(test_dir):\n",
        "    for f_paths in sorted(file_names):\n",
        "        image_list.append(os.path.join(dir_path,f_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_GCKXCjpDgR9"
      },
      "outputs": [],
      "source": [
        "os.makedirs('validation/output',exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMfL_8fgO4b7"
      },
      "outputs": [],
      "source": [
        "for count,im_name in enumerate(image_list):\n",
        "    im = np.load(im_name)\n",
        "    # im = tone_transform(im)    \n",
        "    im = torch.from_numpy(im).unsqueeze(0).permute(0,3,1,2).to(device)\n",
        "    \n",
        "    out = model(im)\n",
        "    out = out[0,...].permute(1,2,0).detach().cpu().numpy()\n",
        "    # out = inv_tone_transform(out)\n",
        "    im  = im[0,...].permute(1,2,0).detach().cpu().numpy()\n",
        "    # im = inv_tone_transform(im)\n",
        "    print(count,file_names[count],im.max(),out.max())\n",
        "    np.save(f'validation/output/{file_names[count]}',out.astype(np.float32))\n",
        "    disp = [im,out]\n",
        "    fig = plt.figure(figsize=(12,12))\n",
        "    for i in range(2):\n",
        "        plt.subplot(1,2,i+1)    # the number of images in the grid is 5*5 (25)\n",
        "        plt.imshow(disp[i])\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cywt-E2zPfAS"
      },
      "outputs": [],
      "source": [
        "cd /content/validation/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdmkqlWXPgzL"
      },
      "outputs": [],
      "source": [
        "!zip /content/drive/MyDrive/2022/ACCV-22/result.zip -r *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXQulZrRPo5J"
      },
      "outputs": [],
      "source": [
        "!wget https://codalab.lisn.upsaclay.fr/my/datasets/download/b549bbbf-7c4e-4ada-bdfb-429523bac284"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSZKy8NTVVlp"
      },
      "outputs": [],
      "source": [
        "!unzip /content/b549bbbf-7c4e-4ada-bdfb-429523bac284"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvXTmjPeWt8U"
      },
      "outputs": [],
      "source": [
        "!pip install pyiqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKOKfCbBVb7n"
      },
      "outputs": [],
      "source": [
        "cd /content/test_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HpdHgg-Vv9b"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.copytree('/content/drive/MyDrive/2022/ACCV-22/checkpoint','/content/checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNVp2NmGCM0p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "udc_torch_competition.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('yolo')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "5060ca89b9238f9a359ed787bf32323f9f640022a3623089513147027d65d6a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

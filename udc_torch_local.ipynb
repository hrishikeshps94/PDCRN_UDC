{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrishikeshps94/PDCRN_UDC/blob/master/udc_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjnNY4UvZ9a9"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install wandb\n",
        "!pip install torchsummary\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49jiz431yh6l"
      },
      "source": [
        "# Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lvqQFWlygtf"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1l_QOnq1Y-O-xPIBu9a_cl5SX8dCalMiF #Link to the dataset in drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTzESnGLzP4T"
      },
      "outputs": [],
      "source": [
        "!unzip /content/ds.zip -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIx9No4jaOrK"
      },
      "source": [
        "##### Library import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TJCaH25saGGA"
      },
      "outputs": [],
      "source": [
        "import argparse,os,wandb,tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import PIL.Image as Image\n",
        "from torchmetrics import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOsPxcNcaT1X"
      },
      "source": [
        "##### Parameter Intilisation\n",
        "* Here we intialise the paramets such as epoch,batch size, train img size etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n2gTfJ7PaRyL"
      },
      "outputs": [],
      "source": [
        "mode = 'train' #['test', 'train']\n",
        "checkpoint_folder ='checkpoint'\n",
        "model_type = 'PDCRN'\n",
        "train_path = '/media/hrishi/data/WORK/RESEARCH/2022/journal-2022/UDC/ds/Poled/train'\n",
        "test_path = '/media/hrishi/data/WORK/RESEARCH/2022/journal-2022/UDC/ds/Poled/val'\n",
        "batch_size = 1\n",
        "epochs = 1000\n",
        "LR = 1e-4\n",
        "num_filters  = 8\n",
        "dilation_rates = (3, 2, 1, 1, 1, 1)\n",
        "nPyramidFilters = 8\n",
        "log_name = 'logger'\n",
        "in_ch = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xyS7DhhaXE-"
      },
      "source": [
        "##### Dataset Intialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ucDaB-ChaWY6"
      },
      "outputs": [],
      "source": [
        "class Custom_Dataset(Dataset):\n",
        "    def __init__(self,root_dir, is_train=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.is_train = is_train\n",
        "        self.hq_im_file_list = []\n",
        "        self.lq_im_file_list = []\n",
        "        for dir_path, _, file_names in os.walk(root_dir):\n",
        "            for f_paths in sorted(file_names):\n",
        "                if dir_path.endswith('HQ'):\n",
        "                    self.hq_im_file_list.append(os.path.join(dir_path,f_paths))\n",
        "                elif dir_path.endswith('LQ'):\n",
        "                    self.lq_im_file_list.append(os.path.join(dir_path,f_paths))\n",
        "        self.tensor_convert = T.ToTensor()\n",
        "        self.train_transform = T.Compose(\n",
        "    [T.RandomCrop((256,256)),T.RandomVerticalFlip(p=0.5),T.RandomHorizontalFlip(p=0.5)\\\n",
        "       ,T.RandomAffine((0,360))])\n",
        "\n",
        "        # self.val_transform = T.Compose([T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    def __len__(self):\n",
        "        return len(self.hq_im_file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_hq_fname = self.hq_im_file_list[idx]\n",
        "        image_lq_fname = self.lq_im_file_list[idx]\n",
        "        hq_image = self.tensor_convert(Image.open(image_hq_fname).convert('RGB')).unsqueeze(dim=0)\n",
        "        lq_image = self.tensor_convert(Image.open(image_lq_fname).convert('RGB')).unsqueeze(dim=0)\n",
        "        concat_img = torch.cat([hq_image,lq_image],dim=0)\n",
        "        if self.is_train:\n",
        "            image = self.train_transform(concat_img)\n",
        "        else:\n",
        "            # image = self.val_transform(concat_img)\n",
        "            image = concat_img\n",
        "        hq_img,lq_img = image.tensor_split(2)\n",
        "        return lq_img.squeeze(0),hq_img.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FtroVaMValU5"
      },
      "outputs": [],
      "source": [
        "train_ds = Custom_Dataset(train_path,is_train=True)\n",
        "train_dataloader = DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=os.cpu_count())\n",
        "val_ds = Custom_Dataset(test_path,is_train=False)\n",
        "val_dataloader = DataLoader(val_ds,batch_size=1,shuffle=False,num_workers=os.cpu_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRMOPpw4arIq"
      },
      "source": [
        "##### DWT and IWT layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "14TCHFuzaoHM"
      },
      "outputs": [],
      "source": [
        "class DWT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DWT,self).__init__()\n",
        "    def forward(self,input):\n",
        "        x01 = input[:,:,0::2,:] / 4.0\n",
        "        x02 = input[:,:,1::2,:] / 4.0\n",
        "        x1 = x01[:, :,:, 0::2]\n",
        "        x2 = x01[:, :,:, 1::2]\n",
        "        x3 = x02[:, :,:, 0::2]\n",
        "        x4 = x02[:, :,:, 1::2]\n",
        "        y1 = x1+x2+x3+x4\n",
        "        y2 = x1-x2+x3-x4\n",
        "        y3 = x1+x2-x3-x4\n",
        "        y4 = x1-x2-x3+x4\n",
        "        return torch.cat([y1, y2, y3, y4], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_iB1SQcoatEm"
      },
      "outputs": [],
      "source": [
        "class IWT(nn.Module):\n",
        "    def __init__(self,scale=2):\n",
        "        super(IWT,self).__init__()\n",
        "        self.upsampler = nn.PixelShuffle(scale)\n",
        "\n",
        "    def kernel_build(self,input_shape):\n",
        "        c = input_shape[1]\n",
        "        out_c = c >> 2\n",
        "        kernel = np.zeros((c, c,1, 1), dtype=np.float32)\n",
        "        for i in range(0, c, 4):\n",
        "            idx = i >> 2\n",
        "            kernel[idx,idx::out_c,0,0]          = [1, 1, 1, 1]\n",
        "            kernel[idx+out_c,idx::out_c,0,0]    = [1,-1, 1,-1]\n",
        "            kernel[idx+out_c*2,idx::out_c,0,0]  = [1, 1,-1,-1]\n",
        "            kernel[idx+out_c*3,idx::out_c,0,0]  = [1,-1,-1, 1]\n",
        "        self.kernel = torch.tensor(data=kernel,dtype=torch.float32,requires_grad=True).to(device)\n",
        "        return None\n",
        "\n",
        "    def forward(self,input):\n",
        "        self.kernel_build(input.shape)\n",
        "        y = nn.functional.conv2d(input,weight = self.kernel, padding='same')\n",
        "        y = self.upsampler(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoMepuWLaxXD"
      },
      "source": [
        "#### Model Components and Model Intialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_Cf0_OM3avTW"
      },
      "outputs": [],
      "source": [
        "class DilationPyramid(nn.Module):\n",
        "    def __init__(self,num_filters,dilation_rates):\n",
        "        super(DilationPyramid,self).__init__()\n",
        "        self.layer_1 = nn.Conv2d(num_filters,num_filters*2,3,padding='same')\n",
        "        self.layer_2 = nn.Conv2d(num_filters*2,num_filters,3,padding='same',dilation=dilation_rates[0])\n",
        "        self.layer_3 = nn.Sequential(*[nn.Conv2d(num_filters,num_filters,3,padding='same',dilation=dil_rate) \\\n",
        "            for dil_rate in dilation_rates[1:]])\n",
        "        self.layer_4 = nn.Conv2d(num_filters*2,num_filters,1,padding='same')\n",
        "    def forward(self,input):\n",
        "        x = self.layer_1(input)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = torch.cat([input,x],dim=1)\n",
        "        out = self.layer_4(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "icnF5kADazPv"
      },
      "outputs": [],
      "source": [
        "class PyramidBlock(nn.Module):\n",
        "    def __init__(self,num_filters,dilation_rates,nPyramidFilters):\n",
        "        super(PyramidBlock,self).__init__()\n",
        "        self.feat_extract = nn.Sequential(*[DilationPyramid(nPyramidFilters,dilation_rates),\\\n",
        "        nn.Conv2d(num_filters,num_filters,3,padding='same')])\n",
        "    def forward(self,input):\n",
        "        x = self.feat_extract(input)*0.1\n",
        "        return input+x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ai8B5lXIa04v"
      },
      "outputs": [],
      "source": [
        "class UDC_Arc(nn.Module):\n",
        "    def __init__(self,in_ch,num_filters,dilation_rates,nPyramidFilters):\n",
        "        super(UDC_Arc,self).__init__()\n",
        "        self.encoder = nn.Sequential(*nn.ModuleList([DWT(),nn.PixelUnshuffle(downscale_factor=2),\\\n",
        "        nn.Conv2d(in_channels=in_ch*4*4,out_channels=num_filters,kernel_size=5,padding='same'),\\\n",
        "        nn.Conv2d(in_channels=num_filters,out_channels=num_filters,kernel_size=3,padding='same'),\\\n",
        "        PyramidBlock(num_filters,dilation_rates,nPyramidFilters),\\\n",
        "        nn.Conv2d(num_filters,num_filters*2,kernel_size=5,stride=2,padding=2),\\\n",
        "        PyramidBlock(num_filters*2,dilation_rates,nPyramidFilters*2),\\\n",
        "        nn.Conv2d(num_filters*2,num_filters*4,kernel_size=5,stride=2,padding=2),\\\n",
        "        PyramidBlock(num_filters*4,dilation_rates,nPyramidFilters*4)\n",
        "        ]))\n",
        "        self.decoder = nn.Sequential(*nn.ModuleList([PyramidBlock(num_filters*4,dilation_rates,nPyramidFilters*4),\\\n",
        "        nn.ConvTranspose2d(num_filters*4,num_filters*2,kernel_size = 4,stride=2,padding=1),\\\n",
        "        PyramidBlock(num_filters*2,dilation_rates,nPyramidFilters*2),\\\n",
        "        nn.ConvTranspose2d(num_filters*2,num_filters,kernel_size = 4,stride=2,padding=1),\\\n",
        "        PyramidBlock(num_filters,dilation_rates,nPyramidFilters),\\\n",
        "        nn.PixelShuffle(upscale_factor=2),nn.Conv2d(num_filters//4,in_ch*4,3,padding='same'),IWT()\n",
        "        ]))\n",
        "    def forward(self,input):\n",
        "        x_enc = self.encoder(input)\n",
        "        x_dec = self.decoder(x_enc)\n",
        "        return x_dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WE1f7XkDa3SJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Layer (type:depth-idx)                        Param #\n",
            "======================================================================\n",
            "├─Sequential: 1-1                             --\n",
            "|    └─DWT: 2-1                               --\n",
            "|    └─PixelUnshuffle: 2-2                    --\n",
            "|    └─Conv2d: 2-3                            9,608\n",
            "|    └─Conv2d: 2-4                            584\n",
            "|    └─PyramidBlock: 2-5                      --\n",
            "|    |    └─Sequential: 3-1                   5,968\n",
            "|    └─Conv2d: 2-6                            3,216\n",
            "|    └─PyramidBlock: 2-7                      --\n",
            "|    |    └─Sequential: 3-2                   23,712\n",
            "|    └─Conv2d: 2-8                            12,832\n",
            "|    └─PyramidBlock: 2-9                      --\n",
            "|    |    └─Sequential: 3-3                   94,528\n",
            "├─Sequential: 1-2                             --\n",
            "|    └─PyramidBlock: 2-10                     --\n",
            "|    |    └─Sequential: 3-4                   94,528\n",
            "|    └─ConvTranspose2d: 2-11                  8,208\n",
            "|    └─PyramidBlock: 2-12                     --\n",
            "|    |    └─Sequential: 3-5                   23,712\n",
            "|    └─ConvTranspose2d: 2-13                  2,056\n",
            "|    └─PyramidBlock: 2-14                     --\n",
            "|    |    └─Sequential: 3-6                   5,968\n",
            "|    └─PixelShuffle: 2-15                     --\n",
            "|    └─Conv2d: 2-16                           228\n",
            "|    └─IWT: 2-17                              --\n",
            "|    |    └─PixelShuffle: 3-7                 --\n",
            "======================================================================\n",
            "Total params: 285,148\n",
            "Trainable params: 285,148\n",
            "Non-trainable params: 0\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "├─Sequential: 1-1                             --\n",
              "|    └─DWT: 2-1                               --\n",
              "|    └─PixelUnshuffle: 2-2                    --\n",
              "|    └─Conv2d: 2-3                            9,608\n",
              "|    └─Conv2d: 2-4                            584\n",
              "|    └─PyramidBlock: 2-5                      --\n",
              "|    |    └─Sequential: 3-1                   5,968\n",
              "|    └─Conv2d: 2-6                            3,216\n",
              "|    └─PyramidBlock: 2-7                      --\n",
              "|    |    └─Sequential: 3-2                   23,712\n",
              "|    └─Conv2d: 2-8                            12,832\n",
              "|    └─PyramidBlock: 2-9                      --\n",
              "|    |    └─Sequential: 3-3                   94,528\n",
              "├─Sequential: 1-2                             --\n",
              "|    └─PyramidBlock: 2-10                     --\n",
              "|    |    └─Sequential: 3-4                   94,528\n",
              "|    └─ConvTranspose2d: 2-11                  8,208\n",
              "|    └─PyramidBlock: 2-12                     --\n",
              "|    |    └─Sequential: 3-5                   23,712\n",
              "|    └─ConvTranspose2d: 2-13                  2,056\n",
              "|    └─PyramidBlock: 2-14                     --\n",
              "|    |    └─Sequential: 3-6                   5,968\n",
              "|    └─PixelShuffle: 2-15                     --\n",
              "|    └─Conv2d: 2-16                           228\n",
              "|    └─IWT: 2-17                              --\n",
              "|    |    └─PixelShuffle: 3-7                 --\n",
              "======================================================================\n",
              "Total params: 285,148\n",
              "Trainable params: 285,148\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = UDC_Arc(in_ch,num_filters,dilation_rates,nPyramidFilters)\n",
        "model = model.to(device)\n",
        "summary(model,input_size=(3,256,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtiEg3URbP9t"
      },
      "source": [
        "#### Save and Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MH38Eqyia404"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(checkpoint_folder,model_type,type='last'):\n",
        "    checkpoint_folder = os.path.join(checkpoint_folder,model_type)\n",
        "    if not os.path.exists(checkpoint_folder):\n",
        "        os.makedirs(checkpoint_folder)\n",
        "    checkpoint_filename = os.path.join(checkpoint_folder,f'{type}.pth')\n",
        "    save_data = {\n",
        "        'step': current_epoch,\n",
        "        f'best_psnr':best_psnr,\n",
        "        f'best_ssim':best_ssim,\n",
        "        'generator_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(save_data, checkpoint_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Vr9Om5pdbXnH"
      },
      "outputs": [],
      "source": [
        "def load_model_checkpoint_for_training(checkpoint_folder,model_type,type ='last'):\n",
        "    checkpoint_folder = os.path.join(checkpoint_folder,model_type)\n",
        "    checkpoint_filename = os.path.join(checkpoint_folder, f'{type}.pth')\n",
        "    if not os.path.exists(checkpoint_filename):\n",
        "        print(\"Couldn't find checkpoint file. Starting training from the beginning.\")\n",
        "        return 0,0\n",
        "    data = torch.load(checkpoint_filename)\n",
        "    current_epoch = data['step']\n",
        "    best_psnr = data['best_psnr']\n",
        "    best_ssim = data['best_ssim']\n",
        "    model.load_state_dict(data['generator_state_dict'])\n",
        "    optimizer.load_state_dict(data['optimizer_state_dict'])\n",
        "    print(f\"Restored model at epoch {current_epoch}.\")\n",
        "    return best_psnr,best_ssim,current_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLZyq9uGbbYF"
      },
      "source": [
        "#### Train and validation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HZtb872cbZWO"
      },
      "outputs": [],
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    for count,(inputs, gt) in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "        inputs = inputs.to(device)\n",
        "        gt = gt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs,gt)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    wandb.log({'train_l1_loss':loss.item()})\n",
        "    wandb.log({'Learning rate':optimizer.param_groups[0]['lr']})\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sAMMNDR1bdvA"
      },
      "outputs": [],
      "source": [
        "def val_epoch(checkpoint_folder,model_type,best_psnr,best_ssim):\n",
        "    model.eval()\n",
        "    for inputs, gt in tqdm.tqdm(val_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        gt = gt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            _ = criterion(outputs,gt)\n",
        "        psnr.update(outputs,gt)\n",
        "        ssim.update(outputs,gt)\n",
        "    wandb.log({'val_psnr':psnr.compute().item()})\n",
        "    wandb.log({'val_ssim':ssim.compute().item()})\n",
        "    val_psnr,val_ssim = psnr.compute().item(),ssim.compute().item()\n",
        "    psnr.reset()\n",
        "    ssim.reset()\n",
        "    if val_psnr>best_psnr:\n",
        "        best_psnr = val_psnr\n",
        "        save_checkpoint(checkpoint_folder,model_type,'best')\n",
        "    else:\n",
        "        save_checkpoint(checkpoint_folder,model_type,'last')\n",
        "    if val_ssim>best_ssim:\n",
        "        best_ssim = val_ssim\n",
        "    print(f'Epoch = {current_epoch} Val best PSNR = {best_psnr},Val best SSIM= {best_ssim},Val current PSNR = {val_psnr},Val currentSSIM= {val_ssim}')\n",
        "    return best_psnr,best_ssim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-In5_u2bhSE"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EPbRXr3vbfe4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/hrishi/data/envs/miniconda3/envs/torchenv/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SSIM` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "current_epoch = 0\n",
        "best_psnr = 0\n",
        "best_ssim = 0\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=(batch_size*0.3)/256, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-4,weight_decay=1e-6)\n",
        "criterion = torch.nn.L1Loss().to(device)\n",
        "psnr  = PeakSignalNoiseRatio().to(device)\n",
        "ssim = StructuralSimilarityIndexMeasure().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow3czNE8bjDg"
      },
      "outputs": [],
      "source": [
        "best_psnr,best_ssim,current_epoch = load_model_checkpoint_for_training(checkpoint_folder,model_type)\n",
        "wandb.init(project=f\"UDC\",name=log_name)\n",
        "for epoch in range(current_epoch,epochs):\n",
        "    current_epoch = epoch\n",
        "    train_epoch()\n",
        "    best_psnr,best_ssim = val_epoch(checkpoint_folder,model_type,best_psnr,best_ssim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(inp):\n",
        "    output = test_model(inp)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dir = '/media/hrishi/data/WORK/RESEARCH/2022/journal-2022/UDC/ds/Poled/val/LQ'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model = UDC_Arc(in_ch,num_filters,dilation_rates,nPyramidFilters).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "6jEqQV5HxKGo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_model_folder = os.path.join(checkpoint_folder,model_type)\n",
        "checkpoint_filename = os.path.join(checkpoint_model_folder, 'best.pth')\n",
        "data = torch.load(checkpoint_filename)\n",
        "test_model.load_state_dict(data['generator_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_list = []\n",
        "for dir_path, _, file_names in os.walk(test_dir):\n",
        "    for f_paths in sorted(file_names):\n",
        "        image_list.append(os.path.join(dir_path,f_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for count,im_name in enumerate(tqdm.tqdm(image_list)):\n",
        "    im = cv2.imread(im_name)\n",
        "    im = to_tensor(im).unsqueeze(0).to(device)\n",
        "    out = test_model(im)\n",
        "    out = out[0,...].permute(1,2,0).detach().cpu().numpy()\n",
        "    im  = im[0,...].permute(1,2,0).detach().cpu().numpy()\n",
        "    disp = [im,out]\n",
        "    fig = plt.figure(figsize=(12,12))\n",
        "    for i in range(2):\n",
        "        plt.subplot(1,2,i+1)    # the number of images in the grid is 5*5 (25)\n",
        "        plt.imshow(disp[i])\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNX0BYdHEWATa8NoVPdIvn8",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "udc_torch.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "interpreter": {
      "hash": "e9224ef34cb0a5a9f529319a8f3c472af375e732c0f5a2408c12cad5c4f84ce0"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('torchenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
